{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.8 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the code in this section. Change the conditional statement X == Y to X < Y or X >Y, and\n",
    " then see what kind of tensor you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_less = X < Y\n",
    "result_less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_greater = X > Y\n",
    "result_greater\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the two tensors that operate by element in the broadcasting mechanism with\n",
    "other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Try loading datasets, e.g., Abalone from the UCI Machine Learning Repository50 and\n",
    "inspect their properties. What fraction of them has missing values? What fraction of\n",
    "the variables is numerical, categorical, or text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'ID': 191, 'type': 'NATIVE', 'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'venue': 'Significance, 2021', 'year': 2021, 'journal': 'Significance, 2021', 'DOI': '1740-9713.01589', 'URL': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n",
      "           name     role         type demographic  \\\n",
      "0  sepal length  Feature   Continuous        None   \n",
      "1   sepal width  Feature   Continuous        None   \n",
      "2  petal length  Feature   Continuous        None   \n",
      "3   petal width  Feature   Continuous        None   \n",
      "4         class   Target  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                                               None    cm             no  \n",
      "1                                               None    cm             no  \n",
      "2                                               None    cm             no  \n",
      "3                                               None    cm             no  \n",
      "4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n"
     ]
    }
   ],
   "source": [
    "#pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "iris = fetch_ucirepo(id=53) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = iris.data.features \n",
    "y = iris.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(iris.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(iris.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length    0\n",
       "sepal width     0\n",
       "petal length    0\n",
       "petal width     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "num_rows, num_columns = X.shape\n",
    "missing_values_fraction = X.isnull().sum().sum() / (num_rows * num_columns)\n",
    "missing_values_fraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: 4\n",
      "Categorical columns: 0\n",
      "Text columns: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical, categorical, and text variables\n",
    "numerical_columns = X.select_dtypes(include=['number']).shape[1]\n",
    "categorical_columns = X.select_dtypes(include=['category', 'object']).shape[1]\n",
    "text_columns = X.applymap(type).eq(str).sum().sum()  # Count text entries\n",
    "\n",
    "print(f\"Numerical columns: {numerical_columns}\")\n",
    "print(f\"Categorical columns: {categorical_columns}\")\n",
    "print(f\"Text columns: {text_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width\n",
       "0             5.1          3.5           1.4          0.2\n",
       "1             4.9          3.0           1.4          0.2\n",
       "2             4.7          3.2           1.3          0.2\n",
       "3             4.6          3.1           1.5          0.2\n",
       "4             5.0          3.6           1.4          0.2\n",
       "..            ...          ...           ...          ...\n",
       "145           6.7          3.0           5.2          2.3\n",
       "146           6.3          2.5           5.0          1.9\n",
       "147           6.5          3.0           5.2          2.0\n",
       "148           6.2          3.4           5.4          2.3\n",
       "149           5.9          3.0           5.1          1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Try indexing and selecting data columns by name rather than by column number. The\n",
    "pandas documentation on indexing has further details on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5.1\n",
      "Name: sepal length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#select single column by name\n",
    "sepal_length = X[\"sepal length\"]\n",
    "print(sepal_length.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width\n",
      "0           5.1          3.5\n"
     ]
    }
   ],
   "source": [
    "#Select Multiple Columns by Name\n",
    "selected_columns = X[[\"sepal length\", \"sepal width\"]]\n",
    "print(selected_columns.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width\n",
      "0           5.1          3.5\n",
      "1           4.9          3.0\n",
      "2           4.7          3.2\n",
      "3           4.6          3.1\n",
      "4           5.0          3.6\n"
     ]
    }
   ],
   "source": [
    "#Select a Range of Rows and Columns\n",
    "subset = X.loc[0:4, [\"sepal length\", \"sepal width\"]]\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.How large a dataset do you think you could load this way? What might be the limitations? Hint: consider the time to read the data, representation, processing, and memory\n",
    "footprint. Try this out on your laptop. What happens if you try it out on a server?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.How would you deal with data that has a very large number of categories? What if the\n",
    "category labels are all unique? Should you include the latter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==>To handle data with a large number of categories, you can group rare categories into an \"Other\" category or combine similar ones using domain knowledge. Use frequency or target encoding instead of one-hot encoding to save memory, or leverage embeddings for high-cardinality features. Dimensionality reduction techniques like PCA or clustering can also help reduce complexity, and sparse data structures are useful for memory efficiency. Avoid including features with all unique labels, such as user or transaction IDs, as they add no generalization value. Instead, derive meaningful features like counts or aggregates if needed. These approaches optimize both memory usage and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.What alternatives to pandas can you think of? How about loading NumPy tensors from\n",
    "a file ? Check out Pillow , the Python Imaging Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 600, 4)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open the image using Pillow\n",
    "img = Image.open(\"1.png\")\n",
    "\n",
    "# Convert the image to a NumPy array (tensor)\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Display the shape of the resulting NumPy array\n",
    "print(img_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.13 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the transpose of the transpose of a matrix is the matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.tensor([[1,2],[3,4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranpose_ofA=A.T\n",
    "tranpose_ofA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranpose_of_transpoe_ofA=tranpose_ofA.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranpose_of_transpoe_ofA==A\n",
    "#proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two matrices A and B, show that sum and transposition commute:\n",
    "tranpose of A + transpose of B= tranpose of (A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "B = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "LHS = A.T + B.T\n",
    "\n",
    "RHS = (A + B).T\n",
    "\n",
    "LHS==RHS\n",
    "#proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any square matrix A, is A + A.T always symmetric? Can you prove the result by\n",
    "using only the results of the previous two exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "\n",
      "A + A^T:\n",
      "tensor([[ 2.,  6., 10.],\n",
      "        [ 6., 10., 14.],\n",
      "        [10., 14., 18.]])\n",
      "\n",
      "Is A + A^T symmetric? True\n"
     ]
    }
   ],
   "source": [
    "#square MATRIX\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "result = A + A.T\n",
    "\n",
    "# Check if the result is symmetric: result == result^T\n",
    "is_symmetric = torch.equal(result, result.T)\n",
    "\n",
    "# Print the results\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nA + A^T:\")\n",
    "print(result)\n",
    "print(\"\\nIs A + A^T symmetric?\", is_symmetric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)?\n",
    "Write your answer without implementing any code, then check your answer using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 3, 4])\n",
      "len(X): 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.randn(2, 3, 4)\n",
    "\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"len(X):\", len(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a tensor X of arbitrary shape, does len(X) always correspond to the length of a\n",
    "certain axis of X? What is that axis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==>Yes, for a tensor 𝑋 of arbitrary shape, the output of \n",
    "len(𝑋)\n",
    "len(X) always corresponds to the length (or size) of the first axis (also known as the 0th dimension) of 𝑋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run A / A.sum(axis=1) and see what happens. Can you analyze the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[ 6.],\n",
      "        [15.],\n",
      "        [24.]])\n",
      "tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.2667, 0.3333, 0.4000],\n",
      "        [0.2917, 0.3333, 0.3750]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "# Compute row-wise sum\n",
    "row_sums = A.sum(axis=1, keepdim=True)  # Keep dimensions for broadcasting\n",
    "\n",
    "# Normalize rows of A\n",
    "normalized_A = A / row_sums\n",
    "\n",
    "print(A)\n",
    "\n",
    "print(row_sums)\n",
    "\n",
    "print(normalized_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When A/A.sum(axis=1) is computed:\n",
    "\n",
    "Each row of A is normalized so that the sum of its elements equals 1.\n",
    "Each element becomes a proportion of its row's total sum.\n",
    "This is achieved through broadcasting, where the row sums are divided across all elements in the respective rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When traveling between two points in downtown Manhattan, what is the distance that\n",
    "you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can\n",
    "you travel diagonally?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, you cannot travel diagonally in Manhattan since the streets and avenues form a grid. Travel is restricted to horizontal and vertical movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs\n",
    "along axes 0, 1, and 2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([2, 3, 4])\n",
      "Shape after summation along axis 0: torch.Size([3, 4])\n",
      "Shape after summation along axis 1: torch.Size([2, 4])\n",
      "Shape after summation along axis 2: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "\n",
    "# Summation along axes\n",
    "sum_axis_0 = X.sum(axis=0)\n",
    "sum_axis_1 = X.sum(axis=1)\n",
    "sum_axis_2 = X.sum(axis=2)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape after summation along axis 0:\", sum_axis_0.shape)\n",
    "print(\"Shape after summation along axis 1:\", sum_axis_1.shape)\n",
    "print(\"Shape after summation along axis 2:\", sum_axis_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed a tensor with three or more axes to the linalg.norm function and observe its\n",
    "output. What does this function compute for tensors of arbitrary shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.linalg.norm function computes the matrix or vector norm of a tensor, depending on its shape and the specified norm type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor X: tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[12., 13., 14., 15.],\n",
      "         [16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.]]])\n",
      "Shape of X: torch.Size([2, 3, 4])\n",
      "Frobenius norm of X: tensor(65.7571)\n"
     ]
    }
   ],
   "source": [
    "# Define a tensor with three axes (2, 3, 4)\n",
    "X = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "\n",
    "# Compute the Frobenius norm\n",
    "norm = torch.linalg.norm(X)\n",
    "\n",
    "print(\"Tensor X:\", X)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Frobenius norm of X:\", norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sums the squares of all elements in the tensor, then takes the square root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensors of arbitrary shape:\n",
    "\n",
    "The tensor is treated as a flattened 1D array, and the Frobenius norm is computed unless a specific axis or norm type is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to compute the product ABC. Is\n",
    "there any difference in memory footprint and speed, depending on whether you compute\n",
    "(AB)C or A(BC). Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product \n",
    "ABC can be computed either as \n",
    "(AB)C or 𝐴(𝐵𝐶)\n",
    "However, the way the multiplication is performed can have an impact on memory footprint and speed due to the order of operations and the size of intermediate matrices involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Footprint: The memory usage is smaller for \n",
    "(𝐴𝐵)𝐶 since the intermediate matrix is smaller.\n",
    "Speed: \n",
    "(𝐴𝐵)𝐶 is faster because fewer operations are required.\n",
    "In summary, computing \n",
    "(𝐴𝐵)𝐶 is more efficient both in terms of memory and computation speed due to the smaller intermediate matrix and fewer operations involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Is there\n",
    "any difference in speed depending on whether you compute AB or AC>? Why? What\n",
    "changes if you initialize C = B> without cloning memory? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AB is computationally cheaper because it involves fewer operations (800 operations) compared to \n",
    "𝐵𝐶 or 𝐴𝐶\n",
    "When you multiply \n",
    "𝐴𝐵 it’s more efficient since the result has fewer elements, reducing the overall computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory impact: If you transpose \n",
    "𝐵 to form 𝐶\n",
    "you reuse memory without duplicating the data, saving memory but not affecting computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider three matrices, say A, B, C ∈ R\n",
    "100×200. Construct a tensor with three axes by\n",
    "stacking [A, B, C]. What is the dimensionality? Slice out the second coordinate of the\n",
    "third axis to recover B. Check that your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = torch.randn(100, 200)\n",
    "B = torch.randn(100, 200)\n",
    "C = torch.randn(100, 200)\n",
    "\n",
    "\n",
    "tensor = torch.stack([A, B, C])\n",
    "\n",
    "\n",
    "recovered_B = tensor[1]\n",
    "\n",
    "\n",
    "print(torch.allclose(recovered_B, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after stacking A , B and C the resulting tensor has shape of (3,100,200)\n",
    "by accessing tensor[1] we can extract the coordinate of third axis which corresponds to B\n",
    "torch.allclose(recovered_B, B) checks if the sliced tensor matches the original matrix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slice operation correctly recovers \n",
    "from the tensor, and the check will return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
